No comments # in code, don't print the site map. Only show corrected script. Sitemap of Directory: Odin 2
==================================================

[Folder] .
--------------------------------------------------
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\config.json
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\config.py
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\engine.py
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\ODIN.bat
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\__init__.py

[Folder] brain
--------------------------------------------------
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\agenticreason.py
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\ai_memory.py
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\conversation_manager.py
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\init.py
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\memory_handler.py
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\typestreaming.py

[Folder] brain\__pycache__
--------------------------------------------------
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\__pycache__\agenticreason.cpython-311.pyc
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\__pycache__\ai_memory.cpython-311.pyc
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\__pycache__\conversation_manager.cpython-311.pyc
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\__pycache__\manager.cpython-311.pyc
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\__pycache__\memory_handler.cpython-311.pyc

[Folder] gui
--------------------------------------------------
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\app.py
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\chatbot_buttons.py
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\chatbot_fields.py
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\chatbot_ui.py
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\CustomText.py
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\message_parser.py
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\stream_response.py

[Folder] gui\__pycache__
--------------------------------------------------
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\__pycache__\accelerator.cpython-311.pyc
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\__pycache__\app.cpython-311.pyc
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\__pycache__\chatbot_buttons.cpython-311.pyc
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\__pycache__\chatbot_ui.cpython-311.pyc
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\__pycache__\CustomText.cpython-311.pyc
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\__pycache__\message_parser.cpython-311.pyc
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\__pycache__\stream_response.cpython-311.pyc

[Folder] __pycache__
--------------------------------------------------
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\__pycache__\config.cpython-311.pyc
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\__pycache__\dependencies.cpython-311.pyc
  C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\__pycache__\engine.cpython-311.pyc

==================================================

Script Contents
==================================================

# START OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\config.py
import os
import json

config_path = os.path.join(os.path.dirname(__file__), 'config.json')
with open(config_path, 'r', encoding='utf-8') as config_file:
    config = json.load(config_file)

OPEN_ROUTER_API_KEY = config['OPEN_ROUTER_API_KEY']
MODEL_NAME = config['MODEL_NAME']
# END OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\config.py


# START OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\engine.py
import os
import json
import customtkinter as ctk
from gui.app import run_gui_wrapper
from brain.ai_memory import conversation_manager
from brain.agenticreason import AgenticReasoner
from config import OPEN_ROUTER_API_KEY, MODEL_NAME

# Initialize the appearance and theme
ctk.set_appearance_mode("dark")
ctk.set_default_color_theme("blue")

# Set the OpenRouter API key and model name for the original assistant
conversation_manager.set_openrouter_api_key(OPEN_ROUTER_API_KEY)
conversation_manager.set_model_name(MODEL_NAME)

# Initialize the Agentic Reasoner
agentic_reasoner = AgenticReasoner(api_key=OPEN_ROUTER_API_KEY, model_name=MODEL_NAME)

def run_engine():
    """
    Main function to run the Odin engine and GUI.
    """
    root = ctk.CTk()
    root.title("Odin")

    # Set the window size and position
    screen_width = root.winfo_screenwidth()
    screen_height = root.winfo_screenheight()

    window_width = int(screen_width * 0.25)
    window_height = int(screen_height * 0.85)

    window_x = (screen_width - window_width) // 2
    window_y = (screen_height - window_height) // 2

    root.geometry(f"{window_width}x{window_height}+{window_x}+{window_y}")
    root.configure(fg_color="#000000")

    # Create a container for the GUI
    container = ctk.CTkFrame(root, fg_color="#000000", border_width=0)
    container.pack(padx=20, pady=20, fill=ctk.BOTH, expand=True)

    # Run the GUI wrapper
    run_gui_wrapper(container)

    # Start the main loop
    root.mainloop()

def send_message(user_message):
    """
    Process the user message using the following flow:
    1. Original AI Assistant processes the query.
    2. Agentic Reasoner refines the response.
    3. Display the refined response.
    4. Process the response with Word2Vec for memory and embeddings.
    """
    try:
        # Step 1: Original AI Assistant processes the query
        original_response = conversation_manager.process_query(user_message)
        if not original_response:
            raise ValueError("Original AI Assistant failed to generate a response.")

        # Step 2: Agentic Reasoner refines the response
        refined_response = agentic_reasoner.process_query(original_response)

        # Step 3: Display the refined response
        chatbot_ui.display_response({"type": "text", "content": refined_response})

        # Step 4: Process the refined response with Word2Vec
        agentic_reasoner.process_response_with_word2vec(refined_response)

    except Exception as e:
        print(f"Error processing user message: {str(e)}")
        chatbot_ui.display_response({"type": "text", "content": f"Error: {str(e)}"})

if __name__ == "__main__":
    run_engine()
# END OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\engine.py


# START OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\__init__.py

# END OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\__init__.py


# START OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\agenticreason.py
import os
import json
import logging
from openai import OpenAI as Client
from .conversation_manager import ConversationManager
from .memory_handler import MemoryHandler

# Set up logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

class AgenticReasoner:
    def __init__(self, api_key, model_name="gpt-4"):
        """
        Initialize the Agentic Reasoner with an API key and model name.
        """
        self.OPEN_ROUTER_API_KEY = api_key
        self.MODEL_NAME = model_name
        self.client = Client(
            base_url="https://openrouter.ai/api/v1",
            api_key=self.OPEN_ROUTER_API_KEY,
        )
        self.conversation_manager = ConversationManager()
        self.memory_handler = MemoryHandler(self.conversation_manager.memory_dir)
        self.role = None  # Initialize role as None, to be set dynamically
        logging.info("Agentic Reasoner initialized.")

    def analyze_conversation_and_set_role(self, user_message):
        """
        Get the role recommendation from the first AI assistant or use the role from memory.
        """
        # Check if there is a previous row in the memory table
        previous_row = self.conversation_manager.get_previous_conversation_row()
        if previous_row:
            # Use the agentic_role from the previous row
            self.role = previous_row.get("agentic_role")
            logging.info(f"Using agentic_role from previous row: {self.role}")
            print(f"Using agentic_role from previous row: {self.role}")
        else:
            # Get a new role recommendation from the first AI assistant
            role_recommendation = self.conversation_manager.get_role_recommendation(user_message)
            if role_recommendation:
                self.role = role_recommendation
                logging.info(f"Second AI Assistant Role: {self.role}")
                print(f"Second AI Assistant Role: {self.role}")
            else:
                logging.warning("No role recommendation received. Using default role.")
                self.role = "You are an AI assistant that helps refine and improve responses from another AI assistant. Analyze the response and make it more detailed, accurate, or user-friendly."
                print(f"Using default role: {self.role}")

    def process_query(self, user_message):
        """
        Process the user query in the following flow:
        1. Get role recommendation from the first AI assistant or use the role from memory.
        2. Original AI Assistant processes the query.
        3. Agentic Reasoner refines the response using the recommended role.
        4. Display the refined response.
        5. Generate bullet points for the refined response.
        6. Process the refined response with Word2Vec for memory and embeddings.
        7. Save the refined response, bullet points, and role to the database.
        """
        logging.info(f"Processing user query: {user_message}")

        # Step 1: Get role recommendation or use role from memory
        self.analyze_conversation_and_set_role(user_message)

        # Step 2: Original AI Assistant processes the query
        original_response = self.conversation_manager.process_query(user_message)
        if not original_response:
            logging.error("Original AI Assistant failed to generate a response.")
            raise ValueError("Original AI Assistant failed to generate a response.")
        logging.info(f"Original AI Assistant response: {original_response}")

        # Step 3: Agentic Reasoner refines the response using the recommended role
        try:
            completion = self.client.chat.completions.create(
                model=self.MODEL_NAME,
                messages=[
                    {"role": "system", "content": self.role},
                    {"role": "user", "content": original_response}
                ],
                extra_headers={"HTTP-Referer": "your_site_url", "X-Title": "your_app_name"}
            )
            if completion.choices and completion.choices[0].message:
                refined_response = completion.choices[0].message.content
                logging.info(f"Agentic Reasoner refined response: {refined_response}")
            else:
                refined_response = original_response
                logging.warning("Agentic Reasoner failed to refine the response. Using original response.")
        except Exception as e:
            refined_response = original_response
            logging.error(f"Error refining response: {str(e)}")

        # Step 4: Display the refined response
        logging.info(f"Displaying refined response: {refined_response}")

        # Step 5: Generate bullet points for the refined response
        bullet_points = self.generate_bullet_points(refined_response)
        logging.info(f"Generated bullet points: {bullet_points}")

        # Step 6: Process the refined response with Word2Vec for memory and embeddings
        embedding = self.process_response_with_word2vec(refined_response)
        if embedding:
            logging.info(f"Generated embedding: {embedding}")
        else:
            logging.warning("Failed to generate embedding for the response.")

        # Step 7: Save the refined response, bullet points, and role to the database
        self.conversation_manager.append_to_conversation(user_message, refined_response, self.role, bullet_points)
        logging.info("Refined response, bullet points, and role saved to the database.")

        return refined_response

    def generate_bullet_points(self, response):
        """
        Generate bullet points from the refined response.
        """
        sentences = response.split(". ")
        bullet_points = "\n".join([f"- {sentence.strip()}" for sentence in sentences if sentence.strip()])
        return bullet_points

    def process_response_with_word2vec(self, response):
        """
        Process the response with Word2Vec for memory and embeddings.
        Returns the embedding vector.
        """
        try:
            embedding = self.memory_handler.sentence_to_vec(response)
            if embedding:
                logging.info("Response processed with Word2Vec and embedded in memory.")
                return embedding
            else:
                logging.warning("Failed to generate Word2Vec embedding for the response.")
                return None
        except Exception as e:
            logging.error(f"Error processing response with Word2Vec: {str(e)}")
            return None
# END OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\agenticreason.py


# START OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\ai_memory.py
import os
import datetime
import numpy as np
import csv
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from openai import OpenAI as Client
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess
import sqlite3
from .memory_handler import MemoryHandler
from .conversation_manager import ConversationManager

conversation_manager = ConversationManager()

def process_response_with_word2vec(self, response):
    """
    Process the response with Word2Vec for memory and embeddings.
    Returns the embedding vector.
    """
    try:
        # Convert the response to a vector using Word2Vec
        embedding = self.memory_handler.sentence_to_vec(response)
        if embedding:
            logging.info("Response processed with Word2Vec and embedded in memory.")
            return embedding
        else:
            logging.warning("Failed to generate Word2Vec embedding for the response.")
            return None
    except Exception as e:
        logging.error(f"Error processing response with Word2Vec: {str(e)}")
        return None
# END OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\ai_memory.py


# START OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\conversation_manager.py
import os
import datetime
import sqlite3
import logging
import json
import re  # For extracting content between ** **
from openai import OpenAI as Client
from .memory_handler import MemoryHandler

# Set up logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

class ConversationManager:
    def __init__(self):
        """
        Initialize the ConversationManager with default settings.
        """
        self.memory_dir = os.path.join(os.path.dirname(__file__), "Memory")
        os.makedirs(self.memory_dir, exist_ok=True)
        self.conv_folder = None
        self.db_path = None
        self.MODEL_NAME = self._load_model_name()
        self.OPEN_ROUTER_API_KEY = None
        self.client = None
        self.memory_handler = MemoryHandler(self.memory_dir)
        self.init_conversation()

    def _load_model_name(self):
        """
        Load the MODEL_NAME from the config.json file.
        """
        config_path = os.path.join(os.path.dirname(__file__), "..", "config.json")
        try:
            with open(config_path, "r", encoding="utf-8") as config_file:
                config = json.load(config_file)
                return config["MODEL_NAME"]
        except Exception as e:
            logging.error(f"Failed to load MODEL_NAME from config.json: {str(e)}")
            raise ValueError("MODEL_NAME must be defined in config.json")

    def init_conversation(self):
        """
        Initialize the conversation by creating a new database file and setting up the memory handler.
        """
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.conv_folder = os.path.join(self.memory_dir, f"memory_{timestamp}")
        os.makedirs(self.conv_folder, exist_ok=True)
        self.db_path = os.path.join(self.conv_folder, "conversations.db")
        self.memory_handler = MemoryHandler(self.conv_folder)  # Initialize MemoryHandler

        # Initialize the database
        self.init_db()

        # Add the agentic_role column if it doesn't exist
        self.add_agentic_role_column()

        # Initialize Word2Vec model only if it doesn't exist
        if not os.path.exists(self.memory_handler.model_path):
            logging.info("Initializing Word2Vec model with default data.")
            self.memory_handler.load_or_train_word2vec_model()

    def init_db(self):
        """
        Initialize the SQLite database with the necessary table.
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''CREATE TABLE IF NOT EXISTS conversations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT,
                message TEXT,
                message_summary TEXT,
                embedding TEXT,
                agentic_role TEXT  -- New column for Agentic Reasoner role
            )''')
        conn.commit()
        conn.close()

    def add_agentic_role_column(self):
        """
        Add the agentic_role column to the conversations table if it doesn't exist.
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        try:
            cursor.execute("ALTER TABLE conversations ADD COLUMN agentic_role TEXT")
            conn.commit()
            logging.info("Added agentic_role column to the conversations table.")
        except sqlite3.OperationalError as e:
            logging.warning(f"Column agentic_role already exists: {str(e)}")
        finally:
            conn.close()

    def get_previous_conversation_row(self):
        """
        Retrieve the previous row from the conversations table.
        """
        if self.db_path and os.path.exists(self.db_path):
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM conversations ORDER BY timestamp DESC LIMIT 1")
            row = cursor.fetchone()
            conn.close()
            if row:
                return {
                    "id": row[0],
                    "timestamp": row[1],
                    "message": row[2],
                    "message_summary": row[3],
                    "embedding": row[4],
                    "agentic_role": row[5]
                }
        return None

    def set_openrouter_api_key(self, api_key):
        """
        Set the OpenRouter API key and update the client.
        """
        self.OPEN_ROUTER_API_KEY = api_key
        self.update_client()

    def set_model_name(self, model_name):
        """
        Set the model name and update the client.
        """
        self.MODEL_NAME = model_name
        self.update_client()

    def update_client(self):
        """
        Update the OpenAI client with the current API key and model name.
        """
        if self.MODEL_NAME and self.OPEN_ROUTER_API_KEY:
            self.client = Client(
                base_url="https://openrouter.ai/api/v1",
                api_key=self.OPEN_ROUTER_API_KEY,
            )

    def get_role_recommendation(self, user_message):
        """
        Get a role recommendation from the first AI based on the user's message.
        """
        if not self.client:
            logging.error("OpenAI client is not initialized.")
            return None

        try:
            # Summarize the user's message
            summary = self.summarize_message(user_message)

            # Ask the AI to recommend a role based on the summary
            role_prompt = f"Based on the following summary of the user's message, recommend a suitable role for an AI assistant to analyze and respond to it. Return the role in the format **Job Title - Job description.**:\n\nSummary: {summary}"
            completion = self.client.chat.completions.create(
                model=self.MODEL_NAME,
                messages=[
                    {"role": "system", "content": "You are an AI assistant that recommends roles for other AI assistants based on user prompts. Return the role in the format **Job Title - Job description.**."},
                    {"role": "user", "content": role_prompt}
                ],
                extra_headers={"HTTP-Referer": "your_site_url", "X-Title": "your_app_name"}
            )
            if completion.choices and completion.choices[0].message:
                return completion.choices[0].message.content.strip()
            else:
                logging.error("No role recommendation received from the AI.")
                return None
        except Exception as e:
            logging.error(f"Error getting role recommendation: {str(e)}")
            return None

    def extract_content_from_markdown(self, role_text):
        """
        Extract the content between ** ** from the markdown-wrapped text.
        """
        match = re.search(r"\*\*(.*?)\*\*", role_text)
        if match:
            return match.group(1)  # Return the content between ** **
        return role_text  # Fallback if no markdown is found

    def process_query(self, user_message):
        """
        Process a user query and generate a response using the AI model.
        """
        if not self.client:
            logging.error("OpenAI client is not initialized.")
            return "Error: OpenAI client is not initialized."

        # Load the conversation history from the database
        conversation_history = self.load_conversation_from_db()

        # Add the system message at the beginning of the conversation history
        conversation_history.insert(0, {"role": "system", "content": "You are an AI assistant. I will remember our conversation and provide relevant responses based on previous interactions."})

        # Add the user's current message
        conversation_history.append({"role": "user", "content": user_message})

        # Generate a response from the AI model
        try:
            completion = self.client.chat.completions.create(
                model=self.MODEL_NAME,
                messages=conversation_history,
                extra_headers={"HTTP-Referer": "your_site_url", "X-Title": "your_app_name"}
            )
            if completion.choices and completion.choices[0].message:
                response_message = completion.choices[0].message.content
                # Save the full message (User Query + AI Response) to the database
                self.append_to_conversation(user_message, response_message)
                return response_message
            else:
                logging.error("Error processing query: No message found in API response.")
                return None
        except Exception as e:
            logging.error(f"Error processing query: {str(e)}")
            return None

    def append_to_conversation(self, user_query, ai_response=None, agentic_role=None, bullet_points=None):
        """
        Append a new conversation entry to the database.
        Ensure all fields (message_summary, embedding, agentic_role) are populated.
        """
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # Combine the user query and AI response
        combined_message = f"User: {user_query}\nAI: {ai_response}" if ai_response else f"User: {user_query}"

        # Step 1: Generate a summary of the message (bullet points)
        message_summary = self.summarize_message(combined_message)

        # Step 2: Convert the message to a vector using Word2Vec
        embedding = self.memory_handler.sentence_to_vec(combined_message)
        embedding_str = ",".join(map(str, embedding)) if embedding is not None else ""

        # Step 3: Get the agentic role (if not provided)
        if agentic_role is None:
            agentic_role = self.get_role_recommendation(user_query)
            if agentic_role is None:
                agentic_role = "**Default Assistant - Provides general assistance and support**"

        # Extract the content between ** **
        extracted_role = self.extract_content_from_markdown(agentic_role)
        print(extracted_role)  # Print the extracted role and description

        # Step 4: Save the conversation to the database
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("INSERT INTO conversations (timestamp, message, message_summary, embedding, agentic_role) VALUES (?, ?, ?, ?, ?)",
                       (timestamp, combined_message, message_summary, embedding_str, extracted_role))
        conn.commit()
        conn.close()

        # Step 5: Update the Word2Vec model with the new message
        self.memory_handler.update_word2vec_model(combined_message)

    def summarize_message(self, message):
        """
        Summarize the message into bullet points.
        Ensure a summary is always generated, even for short or single-word messages.
        """
        try:
            # Split the message into sentences and format as bullet points
            sentences = message.split(". ")
            bullet_points = "\n".join([f"- {sentence.strip()}" for sentence in sentences if sentence.strip()])
            return bullet_points if bullet_points else f"- {message.strip()}"  # Fallback to the full message
        except Exception as e:
            logging.error(f"Error summarizing message: {str(e)}")
            return f"- {message.strip()}"  # Fallback to the full message

    def load_conversation_from_db(self):
        """
        Load the conversation history from the database.
        """
        conversation_history = []
        if self.db_path and os.path.exists(self.db_path):
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT message FROM conversations ORDER BY timestamp ASC")
            rows = cursor.fetchall()
            for row in rows:
                message = row[0]
                if "\n" in message:
                    user_part, ai_part = message.split("\n", 1)
                    user_part = user_part.replace("User: ", "").strip()
                    ai_part = ai_part.replace("AI: ", "").strip()
                    conversation_history.append({"role": "user", "content": user_part})
                    conversation_history.append({"role": "assistant", "content": ai_part})
                else:
                    user_part = message.replace("User: ", "").strip()
                    conversation_history.append({"role": "user", "content": user_part})
            conn.close()
        return conversation_history

    def clear_conversation(self, new_conversation=False):
        """
        Clear the current conversation and optionally start a new one.
        """
        if self.conv_folder:
            for root, dirs, files in os.walk(self.conv_folder, topdown=False):
                for name in files:
                    os.remove(os.path.join(root, name))
                for name in dirs:
                    os.rmdir(os.path.join(root, name))
            os.rmdir(self.conv_folder)
        
        if new_conversation:
            self.init_conversation()
# END OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\conversation_manager.py


# START OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\init.py

# END OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\init.py


# START OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\memory_handler.py
import os
import logging
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess
import numpy as np

# Set up logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

class MemoryHandler:
    def __init__(self, memory_dir):
        self.memory_dir = memory_dir
        self.model_path = os.path.join(self.memory_dir, "word2vec.model")
        self.word2vec_model = None
        self.load_or_train_word2vec_model()

    def load_or_train_word2vec_model(self):
        """
        Load the Word2Vec model if it exists, otherwise train a new one.
        """
        if os.path.exists(self.model_path):
            logging.info("Loading existing Word2Vec model.")
            self.word2vec_model = Word2Vec.load(self.model_path)
        else:
            logging.info("Training new Word2Vec model.")
            # Train a new Word2Vec model with default data
            sentences = [["default", "sentence", "for", "training"]]
            self.word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)
            self.word2vec_model.save(self.model_path)

    def sentence_to_vec(self, sentence):
        """
        Convert a sentence to a vector using Word2Vec.
        Ensure an embedding is always generated, even for short or single-word messages.
        """
        if not self.word2vec_model:
            logging.error("Word2Vec model is not loaded.")
            return None

        try:
            # Preprocess the sentence into words
            words = simple_preprocess(sentence)
            if not words:  # If no words are found, use the entire sentence as a single word
                words = [sentence.strip()]

            # Generate vectors for each word in the sentence
            vectors = [self.word2vec_model.wv[word] for word in words if word in self.word2vec_model.wv]

            if vectors:
                # Return the average of all word vectors
                return np.mean(vectors, axis=0)
            else:
                # If no valid words are found, return a zero vector
                logging.warning("No valid words found in the sentence for Word2Vec. Returning a zero vector.")
                return np.zeros(self.word2vec_model.vector_size)
        except Exception as e:
            logging.error(f"Error converting sentence to vector: {str(e)}")
            return np.zeros(self.word2vec_model.vector_size)

    def update_word2vec_model(self, sentence):
        """
        Update the Word2Vec model with a new sentence.
        """
        if not self.word2vec_model:
            logging.error("Word2Vec model is not loaded.")
            return

        try:
            # Preprocess the sentence into words
            words = simple_preprocess(sentence)
            if not words:  # If no words are found, use the entire sentence as a single word
                words = [sentence.strip()]

            # Update the Word2Vec model with the new sentence
            self.word2vec_model.build_vocab([words], update=True)
            self.word2vec_model.train([words], total_examples=1, epochs=1)
            self.word2vec_model.save(self.model_path)
            logging.info("Word2Vec model updated with new sentence.")
        except Exception as e:
            logging.error(f"Error updating Word2Vec model: {str(e)}")
# END OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\memory_handler.py


# START OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\typestreaming.py
import os
import datetime
import csv
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from openai import OpenAI as Client

class ConversationEventHandler(FileSystemEventHandler):
    def __init__(self, manager):
        self.manager = manager

    def on_modified(self, event):
        if event.src_path == self.manager.conversation_csv_path:
            self.manager.process_new_messages()

class ConversationManager:
    def __init__(self):
        self.memory_dir = os.path.join(os.path.dirname(__file__), "Memory")
        os.makedirs(self.memory_dir, exist_ok=True)
        self.conv_folder = None
        self.conversation_csv_path = None
        self.context = []
        self.unique_entries = set()
        self.MODEL_NAME = "gpt-4"
        self.OPEN_ROUTER_API_KEY = None
        self.client = None
        self.observer = None
        self.last_modified_time = None

        self.init_conversation()

    def init_conversation(self):
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.conv_folder = os.path.join(self.memory_dir, f"memory_{timestamp}")
        os.makedirs(self.conv_folder, exist_ok=True)
        self.conversation_csv_path = os.path.join(self.conv_folder, "conversations.csv")
        
        if not os.path.exists(self.conversation_csv_path):
            with open(self.conversation_csv_path, "w", encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(["Timestamp", "Message", "Summary"])
        self.load_conversation_from_csv()
        self.start_watching_file()

    def load_conversation_from_csv(self):
        current_modified_time = os.path.getmtime(self.conversation_csv_path)
        if self.last_modified_time is None or self.last_modified_time < current_modified_time:
            self.last_modified_time = current_modified_time
            self.context = []
            self.unique_entries = set()
            if os.path.exists(self.conversation_csv_path):
                with open(self.conversation_csv_path, "r", encoding='utf-8', newline='') as f:
                    reader = csv.reader(f)
                    next(reader)  # Skip header row
                    for row in reader:
                        timestamp = row[0]
                        message = row[1]
                        summary = row[2]
                        entry_key = (timestamp, message)
                        if entry_key not in self.unique_entries:
                            self.unique_entries.add(entry_key)
                            self.context.append((timestamp, message, summary))

    def append_to_conversation(self, user_query, ai_response=None, timestamp=None):
        if not timestamp:
            timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        if ai_response is not None:
            combined_message = f"User: {user_query}\nAI: {ai_response}"
        else:
            combined_message = f"User: {user_query}"
        
        summary = self.summarize(combined_message)
        
        entry_key = (timestamp, combined_message)
        if entry_key not in self.unique_entries:
            self.unique_entries.add(entry_key)
            self.context.append((timestamp, combined_message, summary))
            self.save_conversation_to_csv(combined_message, summary, timestamp)

    def summarize(self, message):
        if len(message.split()) <= 50:
            return message
        
        summary_prompt = f"Summarize the following message in 50 words or less:\n\n{message}"
        
        try:
            completion = self.client.chat.completions.create(
                model=self.MODEL_NAME,
                messages=[
                    {"role": "system", "content": "You are an AI assistant tasked with summarizing messages."},
                    {"role": "user", "content": summary_prompt}
                ],
                extra_headers={
                    "HTTP-Referer": "your_site_url",  # Replace with your site URL
                    "X-Title": "your_app_name",       # Replace with your app name
                }
            )
            if completion.choices and completion.choices[0].message:
                return completion.choices[0].message.content.strip()
            else:
                print("Error summarizing message: No message found in API response.")
                return message
        except Exception as e:
            print(f"Error summarizing message: {str(e)}")
            return message

    def process_query(self, user_message):
        self.load_conversation_from_csv()

        self.append_to_conversation(user_message)

        conversation_history = [
            {"role": "system", "content": "You are an AI assistant. I will remember our conversation and provide relevant responses based on previous interactions."}
        ]

        for timestamp, message, summary in self.context:
            if "\n" in message:
                user_part, ai_part = message.split("\n", 1)
                user_part = user_part.replace("User: ", "").strip()
                ai_part = ai_part.replace("AI: ", "").strip()
                conversation_history.append({"role": "user", "content": user_part})
                conversation_history.append({"role": "assistant", "content": ai_part})
            else:
                user_part = message.replace("User: ", "").strip()
                conversation_history.append({"role": "user", "content": user_part})

        try:
            completion = self.client.chat.completions.create(
                model=self.MODEL_NAME,
                messages=conversation_history,
                extra_headers={
                    "HTTP-Referer": "your_site_url",  # Replace with your site URL
                    "X-Title": "your_app_name",       # Replace with your app name
                }
            )
            if completion.choices and completion.choices[0].message:
                response_message = completion.choices[0].message.content
                self.append_to_conversation(user_message, response_message)
                return response_message
            else:
                print("Error processing query: No message found in API response.")
                return None
        except Exception as e:
            print(f"Error processing query: {str(e)}")
            return None

    def process_new_messages(self):
        self.load_conversation_from_csv()
        self.cleanup_csv()

    def cleanup_csv(self):
        # Create a temporary file to store valid rows
        temp_csv_path = self.conversation_csv_path + ".tmp"
        with open(self.conversation_csv_path, "r", encoding='utf-8', newline='') as f_in, open(temp_csv_path, "w", encoding='utf-8', newline='') as f_out:
            reader = csv.reader(f_in)
            writer = csv.writer(f_out)
            header = next(reader)
            writer.writerow(header)  # Write the header to the temp file
            
            for row in reader:
                timestamp = row[0]
                message = row[1]
                summary = row[2]
                if "\n" in message:
                    user_part, ai_part = message.split("\n", 1)
                    if user_part.strip().startswith("User: ") and ai_part.strip().startswith("AI: "):
                        writer.writerow(row)
                else:
                    # If the row does not contain a newline, it might be a user message without AI response
                    if message.strip().startswith("User: "):
                        pass  # Skip this row as it's incomplete

        # Replace the original file with the temp file
        os.replace(temp_csv_path, self.conversation_csv_path)
        self.load_conversation_from_csv()

    def clear_conversation(self):
        if self.conv_folder:
            for root, dirs, files in os.walk(self.conv_folder, topdown=False):
                for name in files:
                    os.remove(os.path.join(root, name))
                for name in dirs:
                    os.rmdir(os.path.join(root, name))
            os.rmdir(self.conv_folder)
        
        self.init_conversation()
        self.context = []
        self.unique_entries = set()

    def set_model_name(self, model_name):
        self.MODEL_NAME = model_name
        self.update_client()

    def set_openrouter_api_key(self, api_key):
        self.OPEN_ROUTER_API_KEY = api_key
        self.update_client()

    def update_client(self):
        if self.MODEL_NAME and self.OPEN_ROUTER_API_KEY:
            self.client = Client(
                base_url="https://openrouter.ai/api/v1",
                api_key=self.OPEN_ROUTER_API_KEY,
            )

    def start_watching_file(self):
        if self.conversation_csv_path:
            event_handler = ConversationEventHandler(self)
            self.observer = Observer()
            self.observer.schedule(event_handler, os.path.dirname(self.conversation_csv_path), recursive=False)
            self.observer.start()

    def stop_watching_file(self):
        if self.observer:
            self.observer.stop()
            self.observer.join()

    def save_conversation_to_csv(self, combined_message, summary, timestamp):
        with open(self.conversation_csv_path, "a", encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([timestamp, combined_message, summary])

# Define the conversation manager instance
conversation_manager = ConversationManager()
conversation_manager.set_model_name("gpt-4")  # Set your desired model name
conversation_manager.set_openrouter_api_key("your_openrouter_api_key")  # Set your OpenRouter API key
conversation_manager.init_conversation()

# Example usage:
user_query = "Tell me a story about a brave knight who saved a kingdom from a dragon."
response = conversation_manager.process_query(user_query)
print(f"AI Response: {response}")
# END OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\brain\typestreaming.py


# START OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\app.py
from .chatbot_ui import ChatbotUI

def run_gui_wrapper(container):
    chatbot_ui = ChatbotUI(container)
# END OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\app.py


# START OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\chatbot_buttons.py
import customtkinter as ctk

class ChatbotButtons:
    def __init__(self, master, chatbot_ui):
        self.master = master
        self.chatbot_ui = chatbot_ui

        self.button_frame = ctk.CTkFrame(self.master)
        self.button_frame.pack(expand=True, fill=ctk.X, padx=10, pady=10)

        self.new_conversation_button = ctk.CTkButton(
            self.button_frame, 
            text="New Conversation", 
            command=self.new_conversation, 
            width=180,
            height=50,
            fg_color="#000000",  
            corner_radius=0,     
            font=("Segoe UI", 15)  
        )
        self.new_conversation_button.pack(side=ctk.LEFT, padx=10, pady=10)

        self.clear_chat_button = ctk.CTkButton(
            self.button_frame, 
            text="Clear Chat", 
            command=self.clear_chat, 
            width=180,
            height=50,
            fg_color="#000000",  
            corner_radius=0,     
            font=("Segoe UI", 15)  
        )
        self.clear_chat_button.pack(side=ctk.LEFT, padx=10, pady=10)

        self.update_model_button = ctk.CTkButton(
            self.button_frame, 
            text="Update Model", 
            command=self.update_model, 
            width=180,
            height=50,
            fg_color="#000000",  
            corner_radius=0,     
            font=("Segoe UI", 15)  
        )
        self.update_model_button.pack(side=ctk.LEFT, padx=10, pady=10)

        self.model_name_entry = ctk.CTkEntry(
            self.button_frame, 
            font=("Segoe UI", 18),
            placeholder_text=self.chatbot_ui.conversation_manager.MODEL_NAME,
            width=700 - 60
        )
        self.model_name_entry.pack(side=ctk.LEFT, padx=(10, 10), pady=(10, 10))

    def new_conversation(self):
        # Ensure we properly clear the chat and start new one
        self.chatbot_ui.clear_chat(new_conversation=True)

    def clear_chat(self):
        # Clear chat without starting a new conversation
        self.chatbot_ui.clear_chat(new_conversation=False)

    def update_model(self):
        new_model_name = self.model_name_entry.get().strip()
        if new_model_name:
            self.chatbot_ui.conversation_manager.set_model_name(new_model_name)
            self.chatbot_ui.widgets['text_box'].configure(state="normal")
            self.chatbot_ui.widgets['text_box'].insert('end', f"Model updated to {new_model_name}.\n", "assistant")
            self.chatbot_ui.widgets['text_box'].configure(state="disabled")
            self.chatbot_ui.widgets['text_box'].yview('end')
# END OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\chatbot_buttons.py


# START OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\chatbot_fields.py
import customtkinter as ctk
from .CustomText import CustomText

class ChatbotFields:
    def __init__(self, master, chatbot_ui):
        self.master = master
        self.chatbot_ui = chatbot_ui

        self.text_frame = ctk.CTkFrame(self.master)
        self.text_frame.pack(pady=10, padx=10, fill=ctk.BOTH, expand=True)

        self.text_box = CustomText(self.text_frame)
        self.text_box.pack(pady=10, padx=10, fill='both', expand=True)

        self.scrollbar = ctk.CTkScrollbar(self.text_frame, command=self.text_box.yview)
        self.scrollbar.pack(side='right', fill='y')

        self.text_box.config(yscrollcommand=self.scrollbar.set)

        self.entry_frame = ctk.CTkFrame(self.master)
        self.entry_frame.pack(fill=ctk.X, padx=10, pady=(0, 10))

        self.entry = ctk.CTkTextbox(self.entry_frame, height=100)
        self.entry.pack(pady=10, padx=10, fill=ctk.X, expand=True)
        self.entry.bind("<Return>", self.chatbot_ui.send_message_from_key)
        self.entry.bind("<KeyRelease>", self.chatbot_ui.prevent_multiline)
# END OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\chatbot_fields.py


# START OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\chatbot_ui.py
import customtkinter as ctk
from brain.ai_memory import conversation_manager
import threading
import queue
import logging
from .CustomText import CustomText
from .stream_response import stream_response

# Set up logging
logging.basicConfig(level=logging.DEBUG)

class ChatbotUI:
    def __init__(self, master):
        self.master = master
        self.conversation_manager = conversation_manager
        self.widgets = {
            'text_box': None,
            'scrollbar': None,
            'entry': None,
            'top_frame': None,
            'text_frame': None,
            'entry_frame': None,
        }
        self.response_queue = queue.Queue()
        self.stop_streaming = False
        self.streaming_thread = None
        self.lock = threading.Lock()
        self.create_widgets()

    def create_widgets(self):
        self.initialize_top_frame()
        self.initialize_text_frame()
        self.initialize_entry_frame()

    def initialize_top_frame(self):
        from .chatbot_buttons import ChatbotButtons
        self.widgets['top_frame'] = ctk.CTkFrame(self.master)
        self.widgets['top_frame'].pack(expand=False, fill=ctk.X, padx=(5, 10), pady=10)
        ChatbotButtons(self.widgets['top_frame'], self)

    def initialize_text_frame(self):
        self.widgets['text_frame'] = ctk.CTkFrame(self.master)
        self.widgets['text_frame'].pack(pady=0, padx=10, fill=ctk.BOTH, expand=True)
        self.widgets['text_frame'].grid_rowconfigure(0, weight=1)
        self.widgets['text_frame'].grid_columnconfigure(0, weight=1)
        self.widgets['text_frame'].grid_columnconfigure(1, weight=0)

        self.widgets['text_box'] = CustomText(self.widgets['text_frame'])
        self.widgets['text_box'].grid(row=0, column=0, sticky="nsew")

        self.widgets['scrollbar'] = ctk.CTkScrollbar(self.widgets['text_frame'])
        self.widgets['scrollbar'].grid(row=0, column=1, sticky="ns")
        self.widgets['text_box'].configure(yscrollcommand=self.widgets['scrollbar'].set)

    def initialize_entry_frame(self):
        self.widgets['entry_frame'] = ctk.CTkFrame(self.master)
        self.widgets['entry_frame'].pack(fill=ctk.X, padx=(5, 10), pady=10)

        self.widgets['entry'] = ctk.CTkTextbox(self.widgets['entry_frame'], height=100)
        self.widgets['entry'].pack(pady=0, padx=5, fill=ctk.X, expand=True)

        self.widgets['entry'].bind("<Return>", self.send_message_from_key)
        self.widgets['entry'].bind("<KeyRelease>", self.prevent_multiline)

    def prevent_multiline(self, event):
        if event.keysym == "Return":
            self.widgets['entry'].delete("insert", "end lineend")

    def send_message_from_key(self, event):
        if event.keysym == "Return":
            user_message = self.widgets['entry'].get("1.0", "end-1c").strip()
            if user_message:
                self.send_message(user_message)
                self.widgets['entry'].delete("1.0", "end")

    def send_message(self, user_message):
        self.stop_streaming = True
        if self.streaming_thread and self.streaming_thread.is_alive():
            self.streaming_thread.join()
        self.stop_streaming = False
        self.display_user_message(user_message)
        with self.response_queue.mutex:
            self.response_queue.queue.clear()
        self.streaming_thread = threading.Thread(target=stream_response, args=(self, user_message))
        self.streaming_thread.start()
        self.master.after(100, self.check_response_queue)

    def check_response_queue(self):
        try:
            response = self.response_queue.get(block=False)
            if response is None:
                self.display_response({"type": "text", "content": ""}, end_with_newline=True)
            else:
                if not self.stop_streaming:
                    self.display_response(response, end_with_newline=False)
            self.master.after(100, self.check_response_queue)
        except queue.Empty:
            self.master.after(100, self.check_response_queue)

    def display_user_message(self, user_message):
        with self.lock:
            self.widgets['text_box'].configure(state="normal")
            self.widgets['text_box'].insert('end', f"User: {user_message}\n", "user")
            self.widgets['text_box'].configure(state="disabled")
            self.widgets['text_box'].yview('end')

    def display_response(self, response, end_with_newline=False):
        with self.lock:
            self.widgets['text_box'].configure(state="normal")
            if response["type"] == "text":
                content = response['content'].strip()
                if content:  # Only display non-empty content
                    self.widgets['text_box'].insert('end', f"{content}\n", "assistant")
            elif response["type"] == "code":
                content = response['content'].strip()
                if content:  # Only display non-empty content
                    # Insert code with buttons
                    self.widgets['text_box'].insert_code(content, language=response['language'])
            elif response["type"] == "buttons":
                self.widgets['text_box'].insert('end', f"{response['content']}\n", "buttons")
            self.widgets['text_box'].configure(state="disabled")
            self.widgets['text_box'].yview('end')

    def clear_chat(self, new_conversation=False):
        self.stop_streaming = True
        if self.streaming_thread and self.streaming_thread.is_alive():
            self.streaming_thread.join()
        with self.response_queue.mutex:
            self.response_queue.queue.clear()
        self.conversation_manager.clear_conversation(new_conversation=new_conversation)
        self.widgets['text_box'].configure(state="normal")
        self.widgets['text_box'].delete(1.0, "end")
        self.widgets['text_box'].configure(state="disabled")
        self.widgets['entry'].delete("1.0", "end")

    def stop_current_streaming(self):
        self.stop_streaming = True
        if self.streaming_thread and self.streaming_thread.is_alive():
            self.streaming_thread.join()
# END OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\chatbot_ui.py


# START OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\CustomText.py
import tkinter as tk  # Add this import statement
import tkinter.filedialog as filedialog
import pyperclip
import sys

class CustomText(tk.Text):
    def __init__(self, master=None, **kw):
        super().__init__(master, **kw)
        self.configure(
            bg="#282828",
            fg="#FFFFFF",
            insertbackground="#FFFFFF",
            padx=0,
            pady=0,
            highlightthickness=0,
            relief='flat',
            wrap='word'
        )

        self.tag_configure(
            "user",
            background="#343434",
            foreground="#FFFFFF",
            font=("Segoe UI", 12),
            lmargin1=50,
            lmargin2=50,
            rmargin=100,
            wrap='word',
            spacing1=10,
            spacing2=0,
            spacing3=10
        )

        self.tag_configure(
            "assistant",
            background="#1E1E1E",
            foreground="#FFFFFF",
            font=("Segoe UI", 12),
            lmargin1=50,
            lmargin2=50,
            rmargin=100,
            wrap='word',
            spacing1=10,
            spacing2=0,
            spacing3=10
        )

        self.tag_configure(
            "code",
            background="#000000",
            foreground="#FFFFFF",
            font=("Courier New", 12),
            lmargin1=50,
            lmargin2=50,
            rmargin=100,
            wrap='word',
            spacing1=10,
            spacing2=0,
            spacing3=10
        )

        self.tag_configure(
            "buttons",
            background="#000000",
            foreground="#0078D7",
            font=("Courier New", 12, "underline"),
            lmargin1=50,
            lmargin2=50,
            rmargin=100,
            wrap='none',
            spacing1=10,
            spacing2=0,
            spacing3=10
        )

        self.tag_bind("buttons", "<Button-1>", self.handle_button_click)
        self.tag_bind("buttons", "<Enter>", self.on_button_enter)
        self.tag_bind("buttons", "<Leave>", self.on_button_leave)

    def handle_button_click(self, event):
        index = self.index("@%s,%s" % (event.x, event.y))
        print(f"Clicked at index: {index}")

        # Find the range of the "buttons" tag at the clicked position
        button_start = self.index(f"{index} linestart")
        button_end = self.index(f"{index} lineend")
        
        # Get the full text within the "buttons" tag
        button_text = self.get(button_start, button_end).strip()
        print(f"Button text: {button_text}")

        # Find the adjacent code block
        code_start, code_end = self.tag_prevrange("code", index)
        if code_start and code_end:
            code_content = self.get(code_start, code_end).strip()
            print(f"Code content: {code_content}")

            # Extract the language from the code block (if specified)
            language = self.get_language_from_code_block(code_start)
            print(f"Detected language: {language}")

            # Remove the backtick lines from the code content
            cleaned_code_content = self.remove_backtick_lines(code_content)
            print(f"Cleaned code content: {cleaned_code_content}")

            # Check which button was clicked
            if "Copy" in button_text and self.is_click_on_word(event, "Copy"):
                self.copy_code_to_clipboard(cleaned_code_content)
            elif "Save" in button_text and self.is_click_on_word(event, "Save"):
                self.save_code_to_file(cleaned_code_content, language)
        else:
            print("Error: Could not find the adjacent code block.")

    def is_click_on_word(self, event, word):
        """
        Check if the click occurred on a specific word.
        """
        index = self.index("@%s,%s" % (event.x, event.y))
        clicked_word = self.get(index + " wordstart", index + " wordend").strip()
        return clicked_word == word

    def get_language_from_code_block(self, code_start):
        """
        Extract the language from the code block (if specified).
        """
        # Get the first line of the code block
        first_line = self.get(code_start, f"{code_start} lineend").strip()
        if first_line.startswith("```"):
            # Extract the language (e.g., ```python -> "python")
            language = first_line[3:].strip()
            return language if language else None
        return None

    def remove_backtick_lines(self, code_content):
        """
        Remove the backtick lines from the code content.
        """
        lines = code_content.splitlines()
        cleaned_lines = [line for line in lines if not line.strip().startswith("```")]
        return "\n".join(cleaned_lines)

    def copy_code_to_clipboard(self, code_content):
        pyperclip.copy(code_content)
        print("Codeblock Copied!")

    def save_code_to_file(self, code_content, language=None):
        """
        Save the code block to a file with the appropriate file extension based on the language.
        """
        # Map languages to file extensions
        language_to_extension = {
            "python": ".py",
            "javascript": ".js",
            "java": ".java",
            "html": ".html",
            "css": ".css",
            "c": ".c",
            "cpp": ".cpp",
            "bash": ".sh",
            "sql": ".sql",
            # Add more mappings as needed
        }

        # Default to .txt if no language is specified
        file_extension = language_to_extension.get(language, ".txt")

        # Open a file save dialog with the appropriate file extension
        file_path = filedialog.asksaveasfilename(
            defaultextension=file_extension,
            filetypes=[(f"{language or 'Text'} Files", f"*{file_extension}"), ("All Files", "*.*")]
        )

        if file_path:
            with open(file_path, "w") as file:
                file.write(code_content)
            print(f"Codeblock saved to {file_path}")

    def insert_code(self, code, language=None):
        """
        Insert a code block with "Copy" and "Save" buttons, excluding the backtick lines.
        """
        # Insert the code content without the backtick lines
        self.insert('end', f"{code}\n", "code")
        # Insert the "Copy" and "Save" buttons
        self.insert('end', "[Copy] [Save]\n", ("buttons",))
        self.mark_set("insert", "end-2c")

    def on_button_enter(self, event):
        self.config(cursor="hand2")

    def on_button_leave(self, event):
        self.config(cursor="")
# END OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\CustomText.py


# START OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\message_parser.py
# START OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin 2\gui\message_parser.py

# ==================================================
# CONFIGURATION VARIABLES (Edit these as needed)
# ==================================================
INCLUDE_BACKTICKS = False  # True/False: Include the backtick lines (```) in the code block content
INCLUDE_LANGUAGE = True    # True/False: Include the language specification (e.g., "python") in the code block
INCLUDE_CODE_CONTENT = True  # True/False: Include the actual code content in the code block

# ==================================================
# MESSAGE PARSER CLASS
# ==================================================

class MessageParser:
    def __init__(self):
        """
        Initialize the MessageParser with the global configuration variables.
        """
        self.buffer = ""
        self.in_code_block = False
        self.code_language = None
        self.code_block = []

    def parse_response(self, response):
        parsed_messages = []
        # Split the response into lines
        lines = response.split('\n')
        for line in lines:
            if line.strip().startswith('```'):
                # Toggle code block state
                if self.in_code_block:
                    # End of code block
                    code_content = self._build_code_block_content()
                    if code_content:
                        parsed_messages.append({
                            "type": "code",
                            "content": code_content,
                            "language": self.code_language
                        })
                    self.in_code_block = False
                    self.code_language = None
                    self.code_block = []
                else:
                    # Start of code block
                    self.in_code_block = True
                    # Extract language if present
                    lang_part = line.strip().lstrip('```')
                    self.code_language = lang_part if lang_part else None
            elif self.in_code_block:
                # Add line to the current code block
                self.code_block.append(line)
            else:
                # Add text line
                if line.strip():
                    parsed_messages.append({
                        "type": "text",
                        "content": line.strip()
                    })
        return parsed_messages

    def _build_code_block_content(self):
        """
        Build the code block content based on the selected options.

        Returns:
            str: The formatted code block content.
        """
        content = []
        if INCLUDE_CODE_CONTENT and self.code_block:
            if INCLUDE_BACKTICKS:
                # Add backticks if required
                content.append('```' + (self.code_language or ''))
                content.extend(self.code_block)
                content.append('```')
            else:
                content.extend(self.code_block)
        return "\n".join(content) if content else None

# END OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin 2\gui\message_parser.py
# END OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\message_parser.py


# START OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\stream_response.py
from .message_parser import MessageParser
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def stream_response(chatbot_ui, user_message):
    """
    Stream the AI response to the GUI, ensuring code blocks are displayed simply with a black background.
    """
    try:
        # Process the user message and get the AI response
        if not hasattr(chatbot_ui.conversation_manager, 'process_query'):
            logging.error("ConversationManager does not have a 'process_query' method.")
            chatbot_ui.response_queue.put({"type": "text", "content": "Error: ConversationManager is not properly initialized."})
            chatbot_ui.response_queue.put(None)  # Signal end of response
            return

        response = chatbot_ui.conversation_manager.process_query(user_message)
        if not response:
            chatbot_ui.response_queue.put({"type": "text", "content": "Error: No response from the AI model."})
            chatbot_ui.response_queue.put(None)  # Signal end of response
            return

        # Parse the response into messages
        parser = MessageParser()
        parsed_messages = parser.parse_response(response)

        # Send parsed messages to the queue
        for message in parsed_messages:
            chatbot_ui.response_queue.put(message)

        chatbot_ui.response_queue.put(None)  # Signal end of response

    except Exception as e:
        logging.error(f"Error in stream_response: {str(e)}")
        chatbot_ui.response_queue.put({"type": "text", "content": f"Error: {str(e)}"})
        chatbot_ui.response_queue.put(None)  # Signal end of response
# END OF FILE: C:\Users\Sean Craig\Desktop\AI Python Tools\Odin\gui\stream_response.py

